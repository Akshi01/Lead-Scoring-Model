# -*- coding: utf-8 -*-
"""LeadScoringModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/Akshi01/Lead-Scoring-Model/blob/main/LeadScoringModel.ipynb
"""

import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns

from google.colab import files
uploaded = files.upload()

import io
data = pd.read_csv(io.BytesIO(uploaded['Leads X Education.csv']))
# Dataset is now stored in a Pandas Dataframe

data.head()

data.shape

data.info()

data1 = data.copy()

"""Handling null values"""

data1.isnull().sum()

null_values_perc = (data1.isnull().sum()/len(data1))*100
null_values_perc

#dropping columns containing more than 45% null values

null_values_45 = null_values_perc[null_values_perc > 45]
data1.drop(columns = null_values_45.index, inplace=True)
data1.shape

col_few_na = ['Lead Source', 'TotalVisits', 'Page Views Per Visit', 'Last Activity']

data1.dropna(subset = col_few_na, axis = 0, inplace=True)

data1.isnull().sum()

#Handling null values in 'country' column

# if the city is indian then replacing null values in country with India
indian_cities = ['Mumbai', 'Thane & Outskirts', 'Other Cities of Maharashtra']

for i in data1['Country'].isnull().index:
    if data1.loc[i,'City'] in indian_cities:
        data1['Country'] = "India"
    else:
        data1.loc[i,'Country'] = "Unknown"

#Handling null values in 'specialization, How did you hear about X Education, What is your current occupation' columns
fields = ['Specialization', 'How did you hear about X Education', 'What is your current occupation']
data1[fields] = data1[fields].replace("Select", np.nan)
data1[fields] = data1[fields].fillna("NA")

data1['What matters most to you in choosing a course'].value_counts()

tags_stats = data1['Tags'].value_counts()
tags_stats_less_than_50 = tags_stats[tags_stats < 50]

data['Tags'] = data1['Tags'].apply(lambda x: 'Other' if x in tags_stats_less_than_50 else x)
data['Tags'].value_counts()

data1['Tags'] = data1['Tags'].fillna("Unknown")

#city and Lead Profile columns

col = ['City','Lead Profile']
data1[col] = data1[col].fillna("Unknown")

data1 = data1.drop('What matters most to you in choosing a course', axis=1)

data1.isnull().sum()

data2 = data1.copy()

data2.columns

for i in ['Do Not Email', 'Converted','Search','Through Recommendations','A free copy of Mastering The Interview']:
    data2[i] = data2[i].replace("Yes" , 1)
    data2[i] = data2[i].replace("No" , 0)

data2['Do Not Call'].value_counts()

data2['Magazine'].value_counts()

data2['Newspaper Article'].value_counts()

data2['X Education Forums'].value_counts()

data2['Newspaper'].value_counts()

data2['Digital Advertisement'].value_counts()

data2['Receive More Updates About Our Courses'].value_counts()

data2['Get updates on DM Content'].value_counts()

data2['I agree to pay the amount through cheque'].value_counts()

#Removing unneseccary and irrelevant columns

data2.drop(['Prospect ID', 'Lead Number', 'Do Not Call', 'Magazine', 'Newspaper Article','X Education Forums','Newspaper', 'Receive More Updates About Our Courses', 'Get updates on DM Content', 'I agree to pay the amount through cheque'], axis=1, inplace=True)

data3 = data2.copy()

"""EDA"""

sns.countplot( x = data3.Converted, data = data3, palette = sns.color_palette("hls", 8));

origin_count = data3['Lead Origin'].value_counts().sort_values(ascending=False).index
sns.countplot(x = data3['Lead Origin'], data = data3, order = origin_count, palette = sns.color_palette("hls", 8))

plt.xticks(rotation = 45);

source_count = data3['Lead Source'].value_counts().sort_values(ascending=False).index
sns.countplot(x = data3['Lead Source'], data = data3, order = source_count)

plt.xticks(rotation = 75);

data_c = data3[data3['Specialization'] != 'NA']
sns.countplot(x = 'Specialization', data = data_c, order = data_c['Specialization'].value_counts().index)
plt.title("Specialization of leads")
plt.xticks(rotation = 90);

activity = data3['Last Activity'].value_counts().index
sns.countplot(x = 'Last Activity', data=data3, order = activity)
plt.xticks(rotation=90);

occupation = data3['What is your current occupation'].value_counts().index
sns.countplot(x = 'What is your current occupation', data=data3, order = occupation, palette=sns.color_palette("hls", 8))
plt.xticks(rotation=90);

data_new = data3[data3['City'] != 'Select']
city_count = data_new['City'].value_counts().index
sns.countplot(x = 'City', data=data_new, order = city_count, palette=sns.color_palette("hls", 8))
plt.xticks(rotation=70);

sns.countplot(x = 'Lead Origin', data = data3, hue = 'Converted', palette=sns.color_palette("Paired"))
plt.xticks(rotation = 45)
plt.title("Distribution of Lead Origin by Conversion");

sns.countplot(x = 'Lead Source', data = data3, hue = 'Converted', order = data3['Lead Source'].value_counts().index, palette=sns.color_palette("Paired"))
plt.xticks(rotation = 90)
plt.title("Distribution of Lead Source by conversion");

sns.countplot(x = 'Last Notable Activity', data = data3, hue = 'Converted', order = data3['Last Notable Activity'].value_counts().index, palette=sns.color_palette("Paired"))
plt.xticks(rotation = 90)
plt.title("Distribution of leads' last notable activity by conversion");

data3.head()

data3.columns

"""Encoding"""

#applying ohe on the remaining categorical columns

# Select all categorical columns
cat_columns = data3.select_dtypes(include=['object']).columns

# Apply one-hot encoding to categorical columns
data_encoded = pd.get_dummies(data3, columns=cat_columns)
data_encoded.shape

data_encoded1 = data_encoded.copy()

X = data_encoded.drop('Converted', axis=1)
y = data_encoded['Converted']

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# create the Normalizer object
scaler = StandardScaler()

# fit transform the data
X_scaled = scaler.fit_transform(X)

data = pd.DataFrame(X_scaled)


X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2, random_state=42)

"""Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix

lr = LogisticRegression(max_iter = 1000)

lr.fit(X_train, y_train)
pred_prob1 = lr.predict_proba(X_test)[:,1]

threshold = 0.5
y_pred_lr = (pred_prob1 > threshold).astype(int)

accuracy = lr.score(X_test, y_test)
print("Accuracy: ", accuracy)

precision = precision_score(y_test, y_pred_lr)
print("precision: ", precision )

recall = recall_score(y_test, y_pred_lr)
print("Recall: ", recall)

f1 = f1_score(y_test, y_pred_lr)
print("F1 score: ", f1)

cm1 = confusion_matrix(y_test, y_pred_lr)
print(cm1)

sns.heatmap(cm1, annot = True, cmap=sns.color_palette("flare", as_cmap=True))
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.title('Confusion Matrix')
plt.show();

"""Random Forest"""

from sklearn.ensemble import RandomForestClassifier

rf = RandomForestClassifier()
rf.fit(X_train, y_train)

y_pred_rf = lr.predict(X_test)

accuracy_rf = rf.score(X_test, y_test)
print("Accuracy: ", accuracy_rf)

precision_rf = precision_score(y_test, y_pred_rf)
print("precision: ", precision_rf )

recall_rf = recall_score(y_test, y_pred_rf)
print("Recall: ", recall_rf)

f1_rf = f1_score(y_test, y_pred_rf)
print("F1 score: ", f1_rf)

cm2 = confusion_matrix(y_test, y_pred_rf)
print(cm2)

sns.heatmap(cm2, annot = True, cmap=sns.color_palette("flare", as_cmap=True))
plt.xlabel('Predicted label')
plt.ylabel('True label')
plt.title('Confusion Matrix')
plt.show();

from sklearn.model_selection import GridSearchCV

#Fit logistic model
parameters_lr = {'penalty': ['l1', 'l2'], 'C' : np.logspace(-3,3,5,base=10.0)}
lr1 = LogisticRegression(solver='liblinear', random_state=123)

lr_cv = GridSearchCV(lr1, param_grid=parameters_lr, cv=5, scoring='roc_auc', n_jobs=-1)
lr_cv.fit(X_train, y_train)

print(lr_cv.best_params_)
lr_best = lr_cv.best_estimator_

#Fit random forest classifier w/ hyperparameter tuning
parameters_rf = {'max_depth':np.arange(6,30,2),'min_samples_leaf':np.arange(100,500,50)}
rf1 = RandomForestClassifier()

rf_cv = GridSearchCV(rf1, param_grid=parameters_rf, cv=5, scoring='roc_auc', n_jobs=-1)
rf_cv.fit(X_train, y_train)

print(rf_cv.best_params_)
rf_best = rf_cv.best_estimator_

from sklearn.metrics import roc_curve, auc

fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(16, 5))
models = {'Logistic Regression':lr_best, 'Random Forest Classifier':rf_best}

for n,m in models.items():
    # Get predicted probabilities for the test set
    y_pred_proba = m.predict_proba(X_test)[:,1]

    # Calculate the false positive rate, true positive rate, and thresholds
    fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)

    # Calculate the area under the curve (AUC)
    roc_auc = auc(fpr, tpr)

    # Plot the ROC curve
    axes[list(models.keys()).index(n)].plot(fpr, tpr, color='seagreen', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)
    axes[list(models.keys()).index(n)].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    axes[list(models.keys()).index(n)].set_xlim([0.0, 1.0])
    axes[list(models.keys()).index(n)].set_ylim([0.0, 1.05])
    axes[list(models.keys()).index(n)].set_xlabel('False Positive Rate')
    axes[list(models.keys()).index(n)].set_ylabel('True Positive Rate')
    axes[list(models.keys()).index(n)].set_title('ROC Curve - ' + n)
    axes[list(models.keys()).index(n)].legend(loc="lower right")

plt.show()

"""The Logistic Regression model performs slightly better as it shows a higher curve than Random Forest Classifier"""